{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981b478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers\n",
    "!pip install torch\n",
    "!pip install scikit-learn\n",
    "!pip install pint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07673d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import itertools\n",
    "from pint import UnitRegistry\n",
    "\n",
    "def get_answer_options(model1, model2):\n",
    "    \n",
    "    #Get all keys from the model dictionary\n",
    "    model1_keys = list(model1.keys())\n",
    "    model2_keys = list(model2.keys())\n",
    "\n",
    "    substring = \"indicator_Text_\"\n",
    "\n",
    "    #Use the substring above to find all the indicator_texts\n",
    "    all_indicator_texts_model1 = [i for i in model1_keys if substring in i]\n",
    "    all_indicator_texts_model2 = [j for j in model2_keys if substring in j]\n",
    "    \n",
    "    options1 = []\n",
    "    options2 = []\n",
    "    \n",
    "    for indicator_text in all_indicator_texts_model1:\n",
    "        options1.append(model1[indicator_text])\n",
    "    \n",
    "    for indicator_text in all_indicator_texts_model2:\n",
    "        options2.append(model2[indicator_text])\n",
    "    \n",
    "    return options1, options2\n",
    "\n",
    "\n",
    "# Load the SentenceTransformer model\n",
    "new_model_name = \"paraphrase-MiniLM-L6-v2\"\n",
    "new_model = SentenceTransformer(new_model_name)\n",
    "\n",
    "def calculate_answer_similarity(options1, options2):\n",
    "    # Get SentenceTransformer embeddings for each answer option\n",
    "    embeddings1 = [new_model.encode([option_data1], convert_to_tensor=True).squeeze().detach().numpy() for option_data1 in options1]\n",
    "    embeddings2 = [new_model.encode([option_data2], convert_to_tensor=True).squeeze().detach().numpy() for option_data2 in options2]\n",
    "\n",
    "    match_scores = []\n",
    "    chosen_matches = []\n",
    "    not_matches = []\n",
    "\n",
    "    for i, embedding1 in enumerate(embeddings1):\n",
    "        for j, embedding2 in enumerate(embeddings2):\n",
    "            similarity_score = cosine_similarity(embedding1.reshape(1, -1), embedding2.reshape(1, -1))[0][0]\n",
    "\n",
    "            match_scores.append({'option_1': options1[i], 'option_2': options2[j], 'similarity_score': similarity_score})\n",
    "\n",
    "    sorted_possible_matches = sorted(match_scores, key=lambda x: x['similarity_score'], reverse=True)\n",
    "\n",
    "    chosen_options = set()\n",
    "\n",
    "    for possible_match in sorted_possible_matches:\n",
    "        option_1 = possible_match['option_1']\n",
    "        option_2 = possible_match['option_2']\n",
    "\n",
    "        if possible_match['similarity_score'] >= 0.78:\n",
    "            if option_1 not in chosen_options and option_2 not in chosen_options:\n",
    "                chosen_matches.append((option_1, option_2))\n",
    "                chosen_options.add(option_1)\n",
    "                chosen_options.add(option_2)\n",
    "        else:\n",
    "            if option_1 not in chosen_options:\n",
    "                not_matches.append(option_1)\n",
    "                chosen_options.add(option_1)\n",
    "            if option_2 not in chosen_options:\n",
    "                not_matches.append(option_2)\n",
    "                chosen_options.add(option_2)\n",
    "\n",
    "    return chosen_matches, not_matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ce51770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 empty topic descriptions. Do you want to use the topic_description attribute? (yes/no): yes\n",
      "There are 0 empty indicator names. Do you want to use indicator_name attribute? (yes/no): no\n",
      "Process completed successfully .\n",
      "CPU times: user 1.73 s, sys: 97.9 ms, total: 1.83 s\n",
      "Wall time: 3.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load the SentenceTransformer model\n",
    "new_model_name = \"paraphrase-MiniLM-L6-v2\"\n",
    "new_model = SentenceTransformer(new_model_name)\n",
    "ureg = UnitRegistry()\n",
    "\n",
    "\n",
    "def generate_back_conversion_description(original_unit, converted_unit):\n",
    "    try:\n",
    "        # Attempt to find a back conversion formula\n",
    "        back_conversion = ureg(original_unit).to(converted_unit)\n",
    "        return f\"Formula: {original_unit} to {converted_unit} = {back_conversion}\"\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def process_transformation_formula(model1, model2, output_file):\n",
    "    chosen_model = choose_model(model1, model2)\n",
    "\n",
    "    name = chosen_model.get('Name', '')\n",
    "    preunit = chosen_model.get('indicator_PreUnit', '')\n",
    "    postunit = chosen_model.get('indicator_PostUnit', '')\n",
    "\n",
    "    preunit_found = False\n",
    "    postunit_found = False\n",
    "\n",
    "    # Adding Formula fragment2 for PreUnit\n",
    "    for model in [model1, model2]:\n",
    "        if model != chosen_model and \"indicator_PreUnit\" in model:\n",
    "            back_conversion_description_preunit = generate_back_conversion_description(\n",
    "                preunit, model.get('indicator_PreUnit', '')\n",
    "            )\n",
    "            if back_conversion_description_preunit:\n",
    "                output_file.write(f'indicator_FormulaIndicator1: {preunit} * 1\"\\n')\n",
    "                output_file.write(f'indicator_FormulaIndicator2: {back_conversion_description_preunit}\"\\n')\n",
    "                preunit_found = True\n",
    "\n",
    "    # Adding Formula fragment2 for PostUnit\n",
    "    for model in [model1, model2]:\n",
    "        if model != chosen_model and \"indicator_PostUnit\" in model:\n",
    "            post_unit_value = model.get('indicator_PostUnit', '')\n",
    "            post_unit_value = post_unit_value.strip('\\\"')  # Remove quotes if present\n",
    "            back_conversion_description_postunit = generate_back_conversion_description(\n",
    "                postunit, post_unit_value\n",
    "            )\n",
    "            if back_conversion_description_postunit:\n",
    "                output_file.write(f'indicator_FormulaIndicator1: \"{postunit} * 1\"\\n')\n",
    "                output_file.write(f'indicator_FormulaIndicator2: \"{back_conversion_description_postunit}\"\\n')\n",
    "                postunit_found = True\n",
    "\n",
    "    # Check if neither preunit nor postunit is found\n",
    "    if not preunit_found and not postunit_found:\n",
    "        output_file.write('indicator_FormulaIndicator1: \"Not found\"\\n')\n",
    "        output_file.write('indicator_FormulaIndicator2: \"Not found\"\\n')\n",
    "\n",
    "    # Separate each pair with an additional newline\n",
    "    output_file.write(\"\\n\")\n",
    "\n",
    "        \n",
    "# Function to calculate similarity score between two \"topic_model\" sections\n",
    "def calculate_similarity_attributes(model1, model2):\n",
    "    # Tokenize and get SentenceTransformer embeddings for each attribute\n",
    "    embeddings1 = {}\n",
    "    embeddings2 = {}\n",
    "    \n",
    "    for attribute in [\"topic_Description\", \"indicator_Name\"]:\n",
    "        value1 = model1[attribute]\n",
    "        value2 = model2[attribute]\n",
    "        \n",
    "        # Obtain embeddings using SentenceTransformer\n",
    "        embedding1 = new_model.encode([value1], convert_to_tensor=True).squeeze().detach().numpy()\n",
    "        embedding2 = new_model.encode([value2], convert_to_tensor=True).squeeze().detach().numpy()\n",
    "        \n",
    "        embeddings1[attribute] = embedding1\n",
    "        embeddings2[attribute] = embedding2\n",
    "    \n",
    "    # Calculate the overall similarity score (average)\n",
    "    similarity_scores = [cosine_similarity([embeddings1[attribute]], [embeddings2[attribute]])[0][0]\n",
    "                         for attribute in [\"topic_Description\", \"indicator_Name\"]]\n",
    "    \n",
    "    overall_similarity = sum(similarity_scores) / len(similarity_scores)\n",
    "    \n",
    "    return overall_similarity, similarity_scores\n",
    "\n",
    "\n",
    "def calculate_similarity_question(model1, model2):\n",
    "    # Tokenize and get SentenceTransformer embeddings for each attribute\n",
    "    embeddings1 = {}\n",
    "    embeddings2 = {}\n",
    "    \n",
    "    for attribute in [\"indicator_Description\"]:\n",
    "        value1 = model1[attribute]\n",
    "        value2 = model2[attribute]\n",
    "        \n",
    "        # Obtain embeddings using SentenceTransformer\n",
    "        embedding1 = new_model.encode([value1], convert_to_tensor=True).squeeze().detach().numpy()\n",
    "        embedding2 = new_model.encode([value2], convert_to_tensor=True).squeeze().detach().numpy()\n",
    "        \n",
    "        embeddings1[attribute] = embedding1\n",
    "        embeddings2[attribute] = embedding2\n",
    "    \n",
    "    # Calculate the overall similarity score (average)\n",
    "    similarity_scores = [cosine_similarity([embeddings1[attribute]], [embeddings2[attribute]])[0][0]\n",
    "                         for attribute in [\"indicator_Description\"]]\n",
    "    \n",
    "    overall_similarity = sum(similarity_scores) / len(similarity_scores)\n",
    "    \n",
    "    return overall_similarity, similarity_scores\n",
    "\n",
    "\n",
    "# Function to parse \"fragment_model\" sections from a data file and assign names\n",
    "def parse_fragment_models(file_path, name_prefix):\n",
    "    fragment_models = []\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = file.read()\n",
    "    \n",
    "    # Split data into individual \"fragment_model\" sections\n",
    "    sections = data.split(\"\\n\\n\")  # Assuming sections are separated by blank lines\n",
    "    \n",
    "    for i, section in enumerate(sections):\n",
    "        attributes = {}\n",
    "        lines = section.strip().split(\"\\n\")\n",
    "        for line in lines:\n",
    "            key, value = line.split(\":\", 1)\n",
    "            attributes[key.strip()] = value.strip()\n",
    "        # Assign names with prefixes\n",
    "        attributes[\"name\"] = f\"{name_prefix}{i + 1}\"\n",
    "        fragment_models.append(attributes)\n",
    "    \n",
    "    return fragment_models\n",
    "\n",
    "# Function to check if the combination of data types is valid\n",
    "def is_data_type_valid(model1, model2):\n",
    "    invalid_rules_data_type = [(\"text\", \"date\"), (\"date\", \"text\"),(\"integer\", \"date\"), (\"date\", \"integer\"),\n",
    "                               (\"double\", \"boolean\"), (\"boolean\", \"double\"), (\"integer\", \"boolean\"), (\"boolean\", \"integer\"), \n",
    "                               (\"double\", \"date\"), (\"date\", \"double\"), (\"boolean\", \"date\"), (\"date\", \"boolean\"),\n",
    "                               (\"singleChoice\", \"multipleChoice\"), (\"multipleChoice\", \"singleChoice\")] \n",
    "    data_types = (model1['indicator_DataType'], model2['indicator_DataType'])\n",
    "    \n",
    "    if (data_types in invalid_rules_data_type):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# Function to choose the appropriate model based on data type rules\n",
    "def choose_model(model1, model2):\n",
    "    datatype_rules = {\n",
    "        (\"text\", \"integer\"): model2,\n",
    "        (\"integer\", \"text\"): model1,\n",
    "        (\"text\", \"double\"): model2,\n",
    "        (\"double\", \"text\"): model1,\n",
    "        (\"text\", \"singleChoice\"): model2,\n",
    "        (\"singleChoice\", \"text\"): model1,\n",
    "        (\"text\", \"multipleChoice\"): model2,\n",
    "        (\"multipleChoice\", \"text\"): model1,\n",
    "        (\"double\", \"integer\"): model1,\n",
    "        (\"integer\", \"double\"): model2,\n",
    "        (\"singleChoice\", \"integer\"): model1,\n",
    "        (\"integer\", \"singleChoice\"): model2,\n",
    "        (\"multipleChoice\", \"integer\"): model1,\n",
    "        (\"integer\", \"multipleChoice\"): model2,\n",
    "        (\"singleChoice\", \"double\"): model1,\n",
    "        (\"double\", \"singleChoice\"): model2,\n",
    "        (\"multipleChoice\", \"double\"): model1,\n",
    "        (\"double\", \"multipleChoice\"): model2,\n",
    "        (\"singleChoice\", \"date\"): model1,\n",
    "        (\"date\", \"singleChoice\"): model2,\n",
    "        (\"multipleChoice\", \"date\"): model1,\n",
    "        (\"date\", \"multipleChoice\"): model2,\n",
    "        (\"singleChoice\", \"boolean\"): model2,\n",
    "        (\"boolean\", \"singleChoice\"): model1,\n",
    "        (\"multipleChoice\", \"singleChoice\"): model2,\n",
    "        (\"singleChoice\", \"multipleChoice\"): model1,\n",
    "        (\"text\", \"text\"): model1,\n",
    "        (\"integer\", \"integer\"): model1,\n",
    "        (\"double\", \"double\"): model1,\n",
    "        (\"date\", \"date\"): model1,\n",
    "        (\"boolean\", \"boolean\"): model1,\n",
    "        (\"multipleChoice\", \"multipleChoice\"): model1,\n",
    "        (\"singleChoice\", \"singleChoice\"): model1,\n",
    "        (\"text\", \"boolean\"): model1,\n",
    "        (\"boolean\", \"text\"): model2,\n",
    "        (\"multipleChoice\", \"boolean\"): model1,\n",
    "        (\"boolean\", \"multipleChoice\"): model2,\n",
    "\n",
    "    }\n",
    "    \n",
    "    data_types = (model1['indicator_DataType'], model2['indicator_DataType'])\n",
    "    \n",
    "    if data_types in datatype_rules:\n",
    "        return datatype_rules[data_types]\n",
    "    else:\n",
    "        raise Exception(f'Unknown data type combination found {data_types}')\n",
    "\n",
    "# Function to get formatted string for writing to files\n",
    "def get_formatted_write_string(model):\n",
    "    string_to_write = \"\"\n",
    "    for key in model.keys():\n",
    "        string_to_write += f\"{key}:{model[key]}\\n\"\n",
    "    return string_to_write\n",
    "\n",
    "\n",
    "\n",
    "def match_and_merge_output(data1_path, data2_path, threshold_indicator, threshold_question, all_scores_file_path, merged_file_path, matched_file_path, unmatched_file_path, use_attribute_topic=True, use_attribute_name=True):\n",
    "    fragment_models1 = parse_fragment_models(data1_path, \"Model_A\")\n",
    "    fragment_models2 = parse_fragment_models(data2_path, \"Model_B\")\n",
    "\n",
    "    # Count of empty indicator descriptions\n",
    "    empty_topic_description_count = sum(not model[\"topic_Description\"] for model in itertools.chain(fragment_models1, fragment_models2))\n",
    "    empty_indicator_count = sum(not model[\"indicator_Name\"] for model in itertools.chain(fragment_models1, fragment_models2))\n",
    "\n",
    "    # Prompt user if they want to use topic attributes \n",
    "    user_input_attribute_topic = input(f\"There are {empty_topic_description_count} empty topic descriptions. Do you want to use the topic_description attribute? (yes/no): \").lower()\n",
    "    use_attribute_topic = use_attribute_topic and user_input_attribute_topic == 'yes'\n",
    "    \n",
    "    # Prompt user if they want to use the calculate_similarity_attributes function\n",
    "    user_input_attribute_name = input(f\"There are {empty_indicator_count} empty indicator names. Do you want to use indicator_name attribute? (yes/no): \").lower()\n",
    "    use_attribute_name = use_attribute_name and user_input_attribute_name == 'yes'\n",
    "\n",
    "    # Initialize a list to store matched models\n",
    "    possible_matches = []\n",
    "    chosen_matches = []\n",
    "    \n",
    "    # Open files for writing\n",
    "    with open(all_scores_file_path, \"w\") as all_scores_file, \\\n",
    "            open(merged_file_path, \"w\") as merged_file, \\\n",
    "            open(matched_file_path, \"w\") as matched_file, \\\n",
    "            open(unmatched_file_path, \"w\") as unmatched_file:\n",
    "\n",
    "        # List to store matched models and their similarity scores\n",
    "        matched_models = []\n",
    "        \n",
    "        # Compare and merge \"topic_model\" sections\n",
    "        for model1, model2 in itertools.product(fragment_models1, fragment_models2):\n",
    "            overall_similarity_indicator, attribute_scores_indicator = calculate_similarity_attributes(model1, model2)\n",
    "            overall_similarity_question, attribute_scores_question = calculate_similarity_question(model1, model2)\n",
    "\n",
    "            if use_attribute_topic or use_attribute_name:\n",
    "                if all(score >= threshold_indicator for score in attribute_scores_indicator) and all(\n",
    "                        score >= threshold_question for score in attribute_scores_question):\n",
    "                \n",
    "                    \n",
    "                    # Write all similarity scores to the Similarity_scores.txt file\n",
    "                    all_scores_file.write(f\"Similarity Scores for {model1['name']} and {model2['name']}:\\n\")\n",
    "                    # Calculate similarity scores for attributes using calculate_similarity_attributes function\n",
    "                    overall_similarity_attributes, attribute_scores_attributes = calculate_similarity_attributes(model1, model2)\n",
    "\n",
    "                    if use_attribute_topic:\n",
    "                        for attribute, score in zip([\"topic_Description\"], attribute_scores_attributes):\n",
    "                            all_scores_file.write(f\"{attribute} Score: {score:.2f}\\n\")\n",
    "\n",
    "                    if use_attribute_name:\n",
    "                        for attribute, score in zip([\"indicator_Name\"], attribute_scores_attributes):\n",
    "                            all_scores_file.write(f\"{attribute} Score: {score:.2f}\\n\")\n",
    "\n",
    "                    # Calculate similarity scores for indicator_Description using calculate_similarity_question function\n",
    "                    overall_similarity_question, attribute_scores_question = calculate_similarity_question(model1, model2)\n",
    "                    for attribute, score in zip([\"indicator_Description\"], attribute_scores_question):\n",
    "                        all_scores_file.write(f\"{attribute} Score: {score:.2f}\\n\")\n",
    "\n",
    "                    # Calculate the average of the individual scores\n",
    "                    average_individual_scores = sum(attribute_scores_attributes + attribute_scores_question) / (\n",
    "                            len(attribute_scores_attributes) + len(attribute_scores_question))\n",
    "                    all_scores_file.write(f\"Overall Similarity (Average of Individual Scores): {average_individual_scores:.2f}\\n\\n\")\n",
    "\n",
    "                    # Check if similarity scores meet the threshold for \"topic_Description\", \"indicator_Name\" and \"indicator_Description\"\n",
    "                    if all(score >= threshold_indicator for score in attribute_scores_indicator) and all(\n",
    "                            score >= threshold_question for score in attribute_scores_question):\n",
    "                        # Check if Indicator_types are equal to each other and if the data types are valid\n",
    "                        if is_data_type_valid(model1, model2):\n",
    "                            similarity_score = average_individual_scores\n",
    "\n",
    "                            # Append matched models and their similarity scores to the list\n",
    "                            matched_models.append({'model_A': model1, 'model_B': model2, 'similarity_score': similarity_score})\n",
    "\n",
    "            else:\n",
    "                # Calculate similarity scores for indicator_Description using calculate_similarity_question function\n",
    "                if all(score >= threshold_question for score in attribute_scores_question):\n",
    "                    # Write all similarity scores to the Similarity_scores.txt file\n",
    "                    all_scores_file.write(f\"Similarity Scores for {model1['name']} and {model2['name']}:\\n\")\n",
    "                    # Calculate similarity scores for indicator_Description using calculate_similarity_question function\n",
    "                    for attribute, score in zip([\"indicator_Description\"], attribute_scores_question):\n",
    "                        all_scores_file.write(f\"{attribute} Score: {score:.2f}\\n\")\n",
    "\n",
    "                    # Calculate the average of the individual scores\n",
    "                    average_individual_scores = sum(attribute_scores_question) / len(attribute_scores_question)\n",
    "\n",
    "                    all_scores_file.write(f\"Overall Similarity (Average of Individual Scores): {average_individual_scores:.2f}\\n\\n\")\n",
    "\n",
    "                    # Check if Indicator_types are equal to each other and if the data types are valid\n",
    "                    if is_data_type_valid(model1, model2):\n",
    "                        similarity_score = average_individual_scores\n",
    "                        # Append matched models and their similarity scores to the list\n",
    "                        matched_models.append({'model_A': model1, 'model_B': model2, 'similarity_score': similarity_score})\n",
    "                    else:\n",
    "                        print(\n",
    "                            f\"Do not match: Model 1 datatype: {model1['indicator_DataType']} Model 2 datatype: {model2['indicator_DataType']}\")\n",
    "\n",
    "\n",
    "        # Sort matched models by similarity score in descending order\n",
    "        sorted_possible_matches = sorted(matched_models, key=lambda x: x['similarity_score'], reverse=True)\n",
    "        \n",
    "  \n",
    "        # Iterate through sorted matches and select unique models\n",
    "        for possible_match in sorted_possible_matches:\n",
    "            if not any(possible_match['model_A'] in item or possible_match['model_B'] in item for item in chosen_matches):\n",
    "\n",
    "                \n",
    "                model1 = possible_match['model_A']\n",
    "                model2 = possible_match['model_B']\n",
    "                    \n",
    "                # In case two sorted possible matches have datatype multipleChoice:\n",
    "                if model1['indicator_DataType'] == 'multipleChoice' and model2['indicator_DataType'] == 'multipleChoice':\n",
    "                    options_model1, options_model2 = get_answer_options(model1,model2)\n",
    "                    matches_answers, not_match_answers = calculate_answer_similarity(options_model1, options_model2)\n",
    "\n",
    "\n",
    "                    #Get all keys from the model dictionary\n",
    "                    model_keys = list(model1.keys())\n",
    "\n",
    "                    substring = \"indicator_Text_\"\n",
    "\n",
    "                    #Use the substring above to find all the indicator_texts\n",
    "                    all_indicator_texts = [i for i in model_keys if substring in i]\n",
    "\n",
    "                    \n",
    "                    #Remove all the indicator_texts\n",
    "                    for indicator_text in all_indicator_texts:\n",
    "                        del model1[indicator_text]\n",
    "\n",
    "                    answer_counter = 1\n",
    "                    for possible_answer_of_match in matches_answers:\n",
    "\n",
    "                        chosen_matched_answer = possible_answer_of_match[0]\n",
    "\n",
    "                        key_name = \"indicator_Text_\" + str(answer_counter)\n",
    "\n",
    "                        model1[key_name] = chosen_matched_answer\n",
    "\n",
    "\n",
    "                        answer_counter += 1\n",
    "\n",
    "\n",
    "                    for unmatched_answer in not_match_answers:\n",
    "\n",
    "                        key_name = \"indicator_Text_\" + str(answer_counter)\n",
    "\n",
    "                        model1[key_name] = unmatched_answer\n",
    "\n",
    "                        answer_counter += 1\n",
    "                        \n",
    "                        \n",
    "                    chosen_matches.append((model1, model2))\n",
    "\n",
    "                    \n",
    "                # In case two sorted possible matches have datatype singleChoice:\n",
    "                elif model1['indicator_DataType'] == 'singleChoice' and model2['indicator_DataType'] == 'singleChoice':   \n",
    "                    options_model1, options_model2 = get_answer_options(model1, model2)\n",
    "                    matches_answers, not_match_answers = calculate_answer_similarity(options_model1, options_model2)\n",
    "\n",
    "                    if not bool(not_match_answers):\n",
    "\n",
    "                        #Get all keys from the model dictionary\n",
    "                        model_keys = list(model1.keys())\n",
    "\n",
    "                        substring = \"indicator_Text_\"\n",
    "\n",
    "                        #Use the substring above to find all the indicator_texts\n",
    "                        all_indicator_texts = [i for i in model_keys if substring in i]\n",
    "\n",
    "\n",
    "                        #Remove all the indicator_texts\n",
    "                        for indicator_text in all_indicator_texts:\n",
    "                            del model1[indicator_text]\n",
    "\n",
    "\n",
    "                        answer_counter = 1\n",
    "                        for possible_answer_of_match in matches_answers:\n",
    "                            chosen_matched_answer = possible_answer_of_match[0]\n",
    "\n",
    "                            key_name = \"indicator_Text_\" + str(answer_counter)\n",
    "\n",
    "                            model1[key_name] = chosen_matched_answer\n",
    "\n",
    "                            answer_counter += 1  \n",
    "                    \n",
    "                    \n",
    "                        chosen_matches.append((model1, model2))\n",
    "              \n",
    "                else: chosen_matches.append((model1, model2))\n",
    "                \n",
    "        \n",
    "        # Write matched models to merged_file and matched_file, and unmatched models to unmatched_file\n",
    "        for match in chosen_matches:\n",
    "            chosen_model = choose_model(match[0], match[1])\n",
    "            formatted_string = get_formatted_write_string(chosen_model)\n",
    "            merged_file.write(formatted_string)\n",
    "             #Additional information from \"indicator_Name\" about Method A and Method B in case of a merge is written to the merged file\n",
    "            merged_file.write(f\"indicator_Indicator1: {match[0]['indicator_Name']}\\n\")\n",
    "            merged_file.write(f\"indicator_Indicator2: {match[1]['indicator_Name']}\\n\")\n",
    "            #The process_transformation_formula function is called to process and write transformation formulas to the merged file.\n",
    "            process_transformation_formula(match[0],match[1],merged_file)\n",
    "            matched_file.write(formatted_string)\n",
    "            matched_file.write(f\"indicator_Indicator1: {match[0]['indicator_Name']}\\n\")\n",
    "            matched_file.write(f\"indicator_Indicator2: {match[1]['indicator_Name']}\\n\")\n",
    "            process_transformation_formula(match[0],match[1],matched_file)\n",
    "            matched_file.write(\"\\n\")\n",
    "\n",
    "        for model in itertools.chain(fragment_models1, fragment_models2):\n",
    "            if not any(model in item for item in chosen_matches):\n",
    "                formatted_string = get_formatted_write_string(model)\n",
    "                merged_file.write(formatted_string)\n",
    "                merged_file.write(\"\\n\")\n",
    "                unmatched_file.write(formatted_string)\n",
    "                unmatched_file.write(\"\\n\")\n",
    "                \n",
    "    print(\"Process completed successfully .\")\n",
    "    \n",
    "data1_path = \"/Users/noahritfeld/Documents/Feedbacktest/2024/data1.txt\"\n",
    "data2_path = \"/Users/noahritfeld/Documents/Feedbacktest/2024/data2.txt\"\n",
    "all_scores_file_path = \"/Users/noahritfeld/Documents/Feedbacktest/2024/Similarity_scores.txt\"\n",
    "merged_file_path = \"/Users/noahritfeld/Documents/Feedbacktest/2024/Merged_models.txt\"\n",
    "matched_file_path = \"/Users/noahritfeld/Documents/Feedbacktest/2024/Matched.txt\"\n",
    "unmatched_file_path = \"/Users/noahritfeld/Documents/Feedbacktest/2024/Unmatched.txt\"\n",
    "    \n",
    "use_attribute_topic = True \n",
    "\n",
    "use_attribute_name = True \n",
    "\n",
    "threshold_indicator = 0.64\n",
    "threshold_question = 0.78\n",
    "\n",
    "# Call the main function to perform matching and write output to files\n",
    "match_and_merge_output(data1_path, data2_path, threshold_indicator, threshold_question, all_scores_file_path, merged_file_path, matched_file_path, unmatched_file_path, use_attribute_topic, use_attribute_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f8d27b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
